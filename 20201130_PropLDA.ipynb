{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoding Variational Inference For Topic Models \n",
    "\n",
    "Paper: https://arxiv.org/pdf/1703.01488.pdf\n",
    "\n",
    "Code in TF: https://akashgit.github.io/autoencoding_vi_for_topic_models/\n",
    "\n",
    "I use the reimplementation in pytorch: https://github.com/hyqneuron/pytorch-avitm/blob/master/pytorch_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import scanpy as sc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"/home/jovyan/data/lung_adult_scATAC/\"\n",
    "experiment_prefix = 'lungAdult_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(outdir + experiment_prefix + '_ATAC_raw.h5ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProdLDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "\n",
    "    def __init__(self, net_arch):\n",
    "        super(ProdLDA, self).__init__()\n",
    "        ac = net_arch\n",
    "        self.net_arch = net_arch\n",
    "        # encoder\n",
    "        self.en1_fc     = nn.Linear(ac.num_input, ac.en1_units)             # 1995 -> 100\n",
    "        self.en2_fc     = nn.Linear(ac.en1_units, ac.en2_units)             # 100  -> 100\n",
    "        self.en2_drop   = nn.Dropout(0.2)\n",
    "        self.mean_fc    = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
    "        self.mean_bn    = nn.BatchNorm1d(ac.num_topic)                      # bn for mean\n",
    "        self.logvar_fc  = nn.Linear(ac.en2_units, ac.num_topic)             # 100  -> 50\n",
    "        self.logvar_bn  = nn.BatchNorm1d(ac.num_topic)                      # bn for logvar\n",
    "        # z\n",
    "        self.p_drop     = nn.Dropout(0.2)\n",
    "        # decoder\n",
    "        self.decoder    = nn.Linear(ac.num_topic, ac.num_input)             # 50   -> 1995\n",
    "        self.decoder_bn = nn.BatchNorm1d(ac.num_input)                      # bn for decoder\n",
    "        # prior mean and variance as constant buffers\n",
    "        prior_mean   = torch.Tensor(1, ac.num_topic).fill_(0)\n",
    "        prior_var    = torch.Tensor(1, ac.num_topic).fill_(ac.variance)\n",
    "        prior_logvar = prior_var.log()\n",
    "        self.register_buffer('prior_mean',    prior_mean)\n",
    "        self.register_buffer('prior_var',     prior_var)\n",
    "        self.register_buffer('prior_logvar',  prior_logvar)\n",
    "        # initialize decoder weight\n",
    "        if ac.init_mult != 0:\n",
    "            #std = 1. / math.sqrt( ac.init_mult * (ac.num_topic + ac.num_input))\n",
    "            self.decoder.weight.data.uniform_(0, ac.init_mult)\n",
    "        # remove BN's scale parameters\n",
    "        self.logvar_bn .register_parameter('weight', None)\n",
    "        self.mean_bn   .register_parameter('weight', None)\n",
    "        self.decoder_bn.register_parameter('weight', None)\n",
    "        self.decoder_bn.register_parameter('weight', None)\n",
    "\n",
    "    def forward(self, input, compute_loss=False, avg_loss=True):\n",
    "        # compute posterior\n",
    "        en1 = F.softplus(self.en1_fc(input))                            # en1_fc   output\n",
    "        en2 = F.softplus(self.en2_fc(en1))                              # encoder2 output\n",
    "        en2 = self.en2_drop(en2)\n",
    "        posterior_mean   = self.mean_bn  (self.mean_fc  (en2))          # posterior mean\n",
    "        posterior_logvar = self.logvar_bn(self.logvar_fc(en2))          # posterior log variance\n",
    "        posterior_var    = posterior_logvar.exp()\n",
    "        # take sample\n",
    "        eps = Variable(input.data.new().resize_as_(posterior_mean.data).normal_()) # noise\n",
    "        z = posterior_mean + posterior_var.sqrt() * eps                 # reparameterization\n",
    "        p = F.softmax(z)                                                # mixture probability\n",
    "        p = self.p_drop(p)\n",
    "        # do reconstruction\n",
    "        recon = F.softmax(self.decoder_bn(self.decoder(p)))             # reconstructed distribution over vocabulary\n",
    "\n",
    "        if compute_loss:\n",
    "            return recon, self.loss(input, recon, posterior_mean, posterior_logvar, posterior_var, avg_loss)\n",
    "        else:\n",
    "            return recon\n",
    "\n",
    "    def loss(self, input, recon, posterior_mean, posterior_logvar, posterior_var, avg=True):\n",
    "        # NL\n",
    "        NL  = -(input * (recon+1e-10).log()).sum(1)\n",
    "        # KLD, see Section 3.3 of Akash Srivastava and Charles Sutton, 2017, \n",
    "        # https://arxiv.org/pdf/1703.01488.pdf\n",
    "        prior_mean   = Variable(self.prior_mean).expand_as(posterior_mean)\n",
    "        prior_var    = Variable(self.prior_var).expand_as(posterior_mean)\n",
    "        prior_logvar = Variable(self.prior_logvar).expand_as(posterior_mean)\n",
    "        var_division    = posterior_var  / prior_var\n",
    "        diff            = posterior_mean - prior_mean\n",
    "        diff_term       = diff * diff / prior_var\n",
    "        logvar_division = prior_logvar - posterior_logvar\n",
    "        # put KLD together\n",
    "        KLD = 0.5 * ( (var_division + diff_term + logvar_division).sum(1) - self.net_arch.num_topic )\n",
    "        # loss\n",
    "        loss = (NL + KLD)\n",
    "        # in traiming mode, return averaged loss. In testing mode, return individual loss\n",
    "        if avg:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.cuda\n",
    "from pprint import pprint, pformat\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('-f', '--en1-units',        type=int,   default=100)\n",
    "# parser.add_argument('-s', '--en2-units',        type=int,   default=100)\n",
    "# parser.add_argument('-t', '--num-topic',        type=int,   default=50)\n",
    "# parser.add_argument('-b', '--batch-size',       type=int,   default=200)\n",
    "# parser.add_argument('-o', '--optimizer',        type=str,   default='Adam')\n",
    "# parser.add_argument('-r', '--learning-rate',    type=float, default=0.002)\n",
    "# parser.add_argument('-m', '--momentum',         type=float, default=0.99)\n",
    "# parser.add_argument('-e', '--num-epoch',        type=int,   default=80)\n",
    "# parser.add_argument('-q', '--init-mult',        type=float, default=1.0)    # multiplier in initialization of decoder weight\n",
    "# parser.add_argument('-v', '--variance',         type=float, default=0.995)  # default variance in prior normal\n",
    "# parser.add_argument('--start',                  action='store_true')        # start training at invocation\n",
    "# parser.add_argument('--nogpu',                  action='store_true')        # do not use GPU acceleration\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "nogpu = True\n",
    "\n",
    "# # default to use GPU, but have to check if GPU exists\n",
    "# if not nogpu:\n",
    "#     if torch.cuda.device_count() == 0:\n",
    "#         args.nogpu = True\n",
    "\n",
    "def to_onehot(data, min_length):\n",
    "    return np.bincount(data, minlength=min_length)\n",
    "\n",
    "def make_data():\n",
    "    global data_tr, data_te, tensor_tr, tensor_te, vocab, vocab_size\n",
    "    dataset_tr = 'data/20news_clean/train.txt.npy'\n",
    "    data_tr = np.load(dataset_tr)\n",
    "    dataset_te = 'data/20news_clean/test.txt.npy'\n",
    "    data_te = np.load(dataset_te)\n",
    "    vocab = 'data/20news_clean/vocab.pkl'\n",
    "    vocab = pickle.load(open(vocab,'r'))\n",
    "    vocab_size=len(vocab)\n",
    "    #--------------convert to one-hot representation------------------\n",
    "    print('Converting data to one-hot representation')\n",
    "    data_tr = np.array([to_onehot(doc.astype('int'),vocab_size) for doc in data_tr if np.sum(doc)!=0])\n",
    "    data_te = np.array([to_onehot(doc.astype('int'),vocab_size) for doc in data_te if np.sum(doc)!=0])\n",
    "    #--------------print the data dimentions--------------------------\n",
    "    print('Data Loaded')\n",
    "    print('Dim Training Data',data_tr.shape)\n",
    "    print('Dim Test Data',data_te.shape)\n",
    "    #-------------make tensor datasets-------------------------------\n",
    "    tensor_tr = torch.from_numpy(data_tr).float()\n",
    "    tensor_te = torch.from_numpy(data_te).float()\n",
    "    if not nogpu:\n",
    "        tensor_tr = tensor_tr.cuda()\n",
    "        tensor_te = tensor_te.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Â Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adata[0:5000].layers[\"binary_raw\"].toarray()\n",
    "data = data[:,np.array(data.sum(0)!=0)]\n",
    "data_tr_coo = scipy.sparse.coo_matrix(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data_tr_coo.data\n",
    "indices = np.vstack((data_tr_coo.row, data_tr_coo.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = data_tr_coo.shape\n",
    "\n",
    "tensor_tr = torch.sparse.FloatTensor(i, v, torch.Size(shape)).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_tr = torch.from_numpy(data_tr).float()\n",
    "# tensor_te = torch.from_numpy(data_te).float()\n",
    "if not nogpu:\n",
    "    tensor_tr = tensor_tr.cuda()\n",
    "#     tensor_te = tensor_te.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dict2Obj(object):\n",
    "    \"\"\"\n",
    "    Turns a dictionary into a class\n",
    "    \"\"\"\n",
    "    #----------------------------------------------------------------------\n",
    "    def __init__(self, dictionary):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "        \n",
    "    \n",
    "#----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    ball_dict = {\"color\":\"blue\",\n",
    "                 \"size\":\"8 inches\",\n",
    "                 \"material\":\"rubber\"}\n",
    "    ball = Dict2Obj(ball_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser.add_argument('-f', '--en1-units',        type=int,   default=100)\n",
    "# parser.add_argument('-s', '--en2-units',        type=int,   default=100)\n",
    "# parser.add_argument('-t', '--num-topic',        type=int,   default=50)\n",
    "# parser.add_argument('-b', '--batch-size',       type=int,   default=200)\n",
    "# parser.add_argument('-o', '--optimizer',        type=str,   default='Adam')\n",
    "# parser.add_argument('-r', '--learning-rate',    type=float, default=0.002)\n",
    "# parser.add_argument('-m', '--momentum',         type=float, default=0.99)\n",
    "# parser.add_argument('-e', '--num-epoch',        type=int,   default=80)\n",
    "# parser.add_argument('-q', '--init-mult',        type=float, default=1.0)    # multiplier in initialization of decoder weight\n",
    "# parser.add_argument('-v', '--variance',         type=float, default=0.995)  # default variance in prior normal\n",
    "# parser.add_argument('--start',                  action='store_true')        # start training at invocation\n",
    "# parser.add_argument('--nogpu',                  action='store_true')        # do not use GPU acceleration\n",
    "\n",
    "\n",
    "args_dict = {'en1_units':100, 'en2_units':100, 'num_topic':30, 'num_input':1, \n",
    "            'variance':0.995, \"init_mult\":1, 'optimizer':'Adam', 'learning_rate':0.01,\n",
    "            \"momentum\":0.99, 'batch_size':200, \"num_epoch\":50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Dict2Obj(args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_arch = Dict2Obj(args_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_arch = {'en1_units':100, 'en2_units':100, 'num_topic':30, 'num_input':1}\n",
    "net_arch.num_input = data_tr_coo.shape[1]\n",
    "model = ProdLDA(net_arch)\n",
    "if not nogpu:\n",
    "    model = model.cuda()\n",
    "\n",
    "        \n",
    "# make_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_optimizer():\n",
    "#     global optimizer\n",
    "if args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), args.learning_rate, betas=(args.momentum, 0.999))\n",
    "elif args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=args.momentum)\n",
    "else:\n",
    "    assert False, 'Unknown optimizer {}'.format(args.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensor does not have a device (device at /tmp/pip-req-build-w9kte7xz/c10/core/TensorImpl.h:463)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f94496cfead in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x12d3f7 (0x7f9447b6b3f7 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #2: at::Tensor::options() const + 0x6f (0x7f9447b73c1f in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x46 (0x7f9443f48f06 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0xe3ff52 (0x7f944426bf52 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xe3a7b3 (0x7f94442667b3 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: <unknown function> + 0x9d2821 (0x7f9443dfe821 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x9d493c (0x7f9443e0093c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: at::native::batch_norm_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool, double, std::array<bool, 3ul>) + 0x12b (0x7f9443e00fdb in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: <unknown function> + 0xd7502d (0x7f94441a102d in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x287f7c8 (0x7f9445cab7c8 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x25ba6a1 (0x7f94459e66a1 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::NativeBatchNormBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x36e (0x7f94459e6c8e in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ad9797 (0x7f9445f05797 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x172b (0x7f9445f0f06b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x415 (0x7f9445f0fe95 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x4b (0x7f9445f04b1b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x4a (0x7f9447f1cdaa in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc9067 (0x7f9584a9f067 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f95877d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f95874fd88f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-c8ff37cc5570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensor does not have a device (device at /tmp/pip-req-build-w9kte7xz/c10/core/TensorImpl.h:463)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f94496cfead in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x12d3f7 (0x7f9447b6b3f7 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #2: at::Tensor::options() const + 0x6f (0x7f9447b73c1f in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x46 (0x7f9443f48f06 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0xe3ff52 (0x7f944426bf52 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xe3a7b3 (0x7f94442667b3 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: <unknown function> + 0x9d2821 (0x7f9443dfe821 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x9d493c (0x7f9443e0093c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: at::native::batch_norm_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool, double, std::array<bool, 3ul>) + 0x12b (0x7f9443e00fdb in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: <unknown function> + 0xd7502d (0x7f94441a102d in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x287f7c8 (0x7f9445cab7c8 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x25ba6a1 (0x7f94459e66a1 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::NativeBatchNormBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x36e (0x7f94459e6c8e in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ad9797 (0x7f9445f05797 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x172b (0x7f9445f0f06b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x415 (0x7f9445f0fe95 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x4b (0x7f9445f04b1b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x4a (0x7f9447f1cdaa in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc9067 (0x7f9584a9f067 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f95877d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f95874fd88f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(args.num_epoch):\n",
    "    all_indices = torch.randperm(tensor_tr.size(0)).split(args.batch_size)\n",
    "    loss_epoch = 0.0\n",
    "    model.train()                   # switch to training mode\n",
    "    for batch_indices in all_indices:\n",
    "        if not nogpu: batch_indices = batch_indices.cuda()\n",
    "        input = Variable(tensor_tr[batch_indices])\n",
    "        recon, loss = model(input, compute_loss=True)\n",
    "        # optimize\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        loss.backward()             # backprop\n",
    "        optimizer.step()            # update parameters\n",
    "        # report\n",
    "        loss_epoch += loss.data[0]    # add loss to loss_epoch\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}, loss={}'.format(epoch, loss_epoch / len(all_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensor does not have a device (device at /tmp/pip-req-build-w9kte7xz/c10/core/TensorImpl.h:463)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f94496cfead in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x12d3f7 (0x7f9447b6b3f7 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #2: at::Tensor::options() const + 0x6f (0x7f9447b73c1f in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x46 (0x7f9443f48f06 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0xe3ff52 (0x7f944426bf52 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xe3a7b3 (0x7f94442667b3 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: <unknown function> + 0x9d2821 (0x7f9443dfe821 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x9d493c (0x7f9443e0093c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: at::native::batch_norm_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool, double, std::array<bool, 3ul>) + 0x12b (0x7f9443e00fdb in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: <unknown function> + 0xd7502d (0x7f94441a102d in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x287f7c8 (0x7f9445cab7c8 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x25ba6a1 (0x7f94459e66a1 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::NativeBatchNormBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x36e (0x7f94459e6c8e in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ad9797 (0x7f9445f05797 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x172b (0x7f9445f0f06b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x415 (0x7f9445f0fe95 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x4b (0x7f9445f04b1b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x4a (0x7f9447f1cdaa in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc9067 (0x7f9584a9f067 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f95877d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f95874fd88f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-159fd6a104ad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensor does not have a device (device at /tmp/pip-req-build-w9kte7xz/c10/core/TensorImpl.h:463)\nframe #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) + 0x6d (0x7f94496cfead in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x12d3f7 (0x7f9447b6b3f7 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #2: at::Tensor::options() const + 0x6f (0x7f9447b73c1f in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #3: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x46 (0x7f9443f48f06 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #4: <unknown function> + 0xe3ff52 (0x7f944426bf52 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #5: <unknown function> + 0xe3a7b3 (0x7f94442667b3 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #6: <unknown function> + 0x9d2821 (0x7f9443dfe821 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x9d493c (0x7f9443e0093c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: at::native::batch_norm_backward_cpu(at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, at::Tensor const&, bool, double, std::array<bool, 3ul>) + 0x12b (0x7f9443e00fdb in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: <unknown function> + 0xd7502d (0x7f94441a102d in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x287f7c8 (0x7f9445cab7c8 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0xd5d82c (0x7f944418982c in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0x25ba6a1 (0x7f94459e66a1 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: torch::autograd::generated::NativeBatchNormBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x36e (0x7f94459e6c8e in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ad9797 (0x7f9445f05797 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x172b (0x7f9445f0f06b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x415 (0x7f9445f0fe95 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: torch::autograd::Engine::thread_init(int) + 0x4b (0x7f9445f04b1b in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::python::PythonEngine::thread_init(int) + 0x4a (0x7f9447f1cdaa in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/torch/lib/libtorch_python.so)\nframe #20: <unknown function> + 0xc9067 (0x7f9584a9f067 in /home/jovyan/my-conda-envs/emma_env/lib/python3.7/site-packages/zmq/backend/cython/../../../../.././libstdc++.so.6)\nframe #21: <unknown function> + 0x76db (0x7f95877d46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #22: clone + 0x3f (0x7f95874fd88f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_topic_in_line(line):\n",
    "    topics = []\n",
    "    for topic, keywords in associations.iteritems():\n",
    "        for word in keywords:\n",
    "            if word in line:\n",
    "                topics.append(topic)\n",
    "                break\n",
    "    return topics\n",
    "\n",
    "def print_top_words(beta, feature_names, n_top_words=10):\n",
    "    print '---------------Printing the Topics------------------'\n",
    "    for i in range(len(beta)):\n",
    "        line = \" \".join([feature_names[j] \n",
    "                            for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
    "        topics = identify_topic_in_line(line)\n",
    "        print('|'.join(topics))\n",
    "        print('     {}'.format(line))\n",
    "    print '---------------End of Topics------------------'\n",
    "\n",
    "def print_perp(model):\n",
    "    cost=[]\n",
    "    model.eval()                        # switch to testing mode\n",
    "    input = Variable(tensor_te)\n",
    "    recon, loss = model(input, compute_loss=True, avg_loss=False)\n",
    "    loss = loss.data\n",
    "    counts = tensor_te.sum(1)\n",
    "    avg = (loss / counts).mean()\n",
    "    print('The approximated perplexity is: ', math.exp(avg))\n",
    "\n",
    "def visualize():\n",
    "    global recon\n",
    "    input = Variable(tensor_te[:10])\n",
    "    register_vis_hooks(model)\n",
    "    recon = model(input, compute_loss=False)\n",
    "    remove_vis_hooks()\n",
    "    save_visualization('pytorch_model', 'png')\n",
    "\n",
    "if __name__=='__main__' and args.start:\n",
    "    make_data()\n",
    "    make_model()\n",
    "    make_optimizer()\n",
    "    train()\n",
    "    emb = model.decoder.weight.data.cpu().numpy().T\n",
    "    print_top_words(emb, zip(*sorted(vocab.items(), key=lambda x:x[1]))[0])\n",
    "    print_perp(model)\n",
    "    visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emma_env)",
   "language": "python",
   "name": "emma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
